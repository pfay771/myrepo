Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job            count
-----------  -------
all                1
extract_cds        1
total              2

Select jobs to execute...

[Mon Feb 16 17:14:42 2026]
rule extract_cds:
    output: hcmv_cds.fasta, LastName_PipelineReport.txt
    jobid: 1
    reason: Missing output files: hcmv_cds.fasta
    resources: tmpdir=/tmp

[Mon Feb 16 17:14:43 2026]
Error in rule extract_cds:
    jobid: 1
    output: hcmv_cds.fasta, LastName_PipelineReport.txt

RuleException:
CalledProcessError in file /home/pfay/Pipeline_Project/Snakefile, line 16:
Command 'set -euo pipefail;  /usr/bin/python3 /home/pfay/Pipeline_Project/.snakemake/scripts/tmp4opnkyqf.get_hcmv_data.py' returned non-zero exit status 1.
  File "/home/pfay/Pipeline_Project/Snakefile", line 16, in __rule_extract_cds
  File "/usr/lib/python3.12/concurrent/futures/thread.py", line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2026-02-16T171441.664405.snakemake.log
